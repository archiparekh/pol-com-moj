{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating Audios\n",
    "\n",
    "This file has exploration of audios and caption translation. No formal analyses here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud Translation library\n",
    "from google.cloud import translate\n",
    "\n",
    "# Initialize Translation client\n",
    "def translate_text(text=\"‡§∏‡§∞‡•ç‡§ï‡§æ‡§∞\", lang=\"hi\"):\n",
    "\n",
    "    try:\n",
    "        if lang == 'en':\n",
    "            return ''\n",
    "\n",
    "        \n",
    "        project_id=\"agile-tracker-398919\"\n",
    "        client = translate.TranslationServiceClient()\n",
    "\n",
    "        location = \"global\"\n",
    "\n",
    "        parent = f\"projects/{project_id}/locations/{location}\"\n",
    "\n",
    "        response = client.translate_text(\n",
    "            request={\n",
    "                \"parent\": parent,\n",
    "                \"contents\": [text],\n",
    "                \"mime_type\": \"text/plain\",  # mime types: text/plain, text/html\n",
    "                \"source_language_code\": lang,\n",
    "                \"target_language_code\": \"en-US\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Display the translation for each input text provided\n",
    "        for translation in response.translations:\n",
    "            return translation.translated_text\n",
    "    except:\n",
    "        return \"error\"\n",
    "# translate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = pd.read_csv(\"download_csvs/audio_transcription_75.csv\").drop(columns=[\"Unnamed: 0.1\", \"Unnamed: 0\"])\n",
    "# audios.text.fillna(\"\", inplace=True)  # run this next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios[\"translation\"] = audios.apply(lambda x: translate_text(x.text, x.lang), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios.to_csv(\"download_csvs/audio_translations_75.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some texts were NaN\n",
    "audios.translation = audios.translation.str.replace(\"error\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ‡§ß‡§ø‡§≠‡§æ constable ‡§ï‡§π‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§¨‡•Å‡§Ø‡•Ä ‡§Ö‡§ï‡§≤‡•ç Pr√§sident ‡§™‡•ç‡§∞‡§¶‡•á‡§∂≈Çug‡§∞‡•§ ‡§π‡•Ä ‡§î‡§∞ ‡§¨‡§æ‡§ù‡§™‡§æ ‡§≠‡§æ‡§ú‡§º‡§æ‰∏á‡•Ä ‡§ï‡§Æ‡•ç‡§Ø‡•ã‡§Ç under Brother Bhashpa Jyot drinking one operation in one place. ‡§≠‡•Å‡§ú‡§∞‡§æ‡§§ ‡§ï‡•á ‡§¨‡§æ‡§∞‡§Æ‡•á ‡§ï‡•å‡§ö ‡§ï‡•à‡§π‡•á‡§Ç‡§ó‡•á‡•§'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios.iloc[14].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9rpw0QQErwU5RVAmKGQQIkvlVDXlm7UVQlXN</td>\n",
       "      <td>‡§Æ‡•à‡§Ç ‡§§‡•ã ‡§™‡§ø‡§∏‡•ç‡§≤‡•á ‡§¶‡§ø‡§®‡•ã ‡§®‡§ø‡§Ø‡•ã‡§∞‡•ç‡§ï ‡§ú‡§æ‡§∞‡§æ ‡§•‡§æ, ‡§π‡§æ‡§≤‡§æ‡§Ç‡§ï‡§ø ‡§Ø...</td>\n",
       "      <td>hi</td>\n",
       "      <td>3152924803</td>\n",
       "      <td>I used to live in New York recently, although...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11QA4nnK1AHERwnV6AyyTblPJ0DB1oIN9N7X</td>\n",
       "      <td>‡§ú‡§¨ ‡§Ö‡§™‡§®‡•á ‡§®‡§¶‡•á‡§∞‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Ö‡§ú‡§ø‡§Ø‡§æ‡§∞‡§æ ‡§Æ‡•à‡§Ç ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§ï‡§æ ‡§ó‡•ã‡§≤ ‡§∞...</td>\n",
       "      <td>hi</td>\n",
       "      <td>3143801451</td>\n",
       "      <td>When Ajiara is in my eyes, I have been the go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11QA4nnK1AHERwnV6AyyTblL16BPJeIN9k5J</td>\n",
       "      <td>⁄©⁄æŸà€Å €ÅŸàŸÜ⁄Üÿßÿ¶€å Ÿæÿ± Ÿæ€åŸπŸÜ€í ÿ≥€í ⁄©ÿ®Ÿàÿ™ÿ± ŸÜ€Å€å⁄∫ ÿ®ŸÜ ÿ¨ÿßÿ™ÿß ÿ®...</td>\n",
       "      <td>ur</td>\n",
       "      <td>3168361509</td>\n",
       "      <td>Beating at the top of the nest does not make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9rpw0QQErwU5RVAmKGQQIkvlVDwnOeFV8Von</td>\n",
       "      <td>Thanks for watching!</td>\n",
       "      <td>en</td>\n",
       "      <td>3152593453</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6P0OR443POT74XKY1x22TWkdLBJ7XDFjojEG</td>\n",
       "      <td>‡§Æ‡•á‡§Ç ‡§ó‡•á‡§Ç‡§ó ‡§µ‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§®‡§æ ‡§¨‡§ú‡§Æ‡§æ ‡§∏‡•ã‡§ú ‡§∏‡•á ‡§°‡§∞‡§§‡§æ ‡§ú‡•ã ‡§§‡•Ä‡§∏ ...</td>\n",
       "      <td>hi</td>\n",
       "      <td>3133451477</td>\n",
       "      <td>I wouldn't have attacked gangs, I would have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2r7wN33WrwU5Xd0L7PbbIKZx5rYYj1ipErZ2</td>\n",
       "      <td>‡§ú‡•ã ‡§≤‡§ø‡§è ‡§ñ‡•Å‡§∞‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§ú‡§æ‡§¶‡§æ ‡§¨‡§æ‡§∞ ‡§Ö‡§≤‡•ç‡§≤‡§æ ‡§ï‡§æ ‡§®‡§æ‡§µ ‡§π...</td>\n",
       "      <td>hi</td>\n",
       "      <td>3139094516</td>\n",
       "      <td>For which in the Quran, Allah's name is menti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4rAQ0LLGrQUKdw6yVXpphPlydVrAbdCN1bYR</td>\n",
       "      <td>ÿßŸèÿ™ÿ± Ÿæÿ±ÿØ€åÿ¥ ŸÖ€å⁄∫ ÿ®ÿ±ŸÑ€å ÿ≥€í ÿØŸÑ€å ÿ®ÿ≥ ÿ¨ÿß ÿ±€Å€å €Å€í ÿ®ÿ≥ ÿ±ÿ≥...</td>\n",
       "      <td>ur</td>\n",
       "      <td>3151930999</td>\n",
       "      <td>In Uttar Pradesh, the bus was going from Burl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6P0OR443POT74XKY1x22TWk8lk7kRYUjpAeL</td>\n",
       "      <td>‡§ú‡§ø‡§®‡•ç‡§¶‡§æ‡§¨‡§æ‡§ú ‡§ú‡§æ‡§ú ‡§¶‡§∞‡§Æ ‡§ï‡•ã ‡§ú‡•ã‡§°‡§º ‡§≠‡§ø‡§ï‡§æ‡§∂ ‡§™‡•á ‡§ì‡§ü ‡§Æ‡§∞‡§æ‡§à ‡§∞‡•á...</td>\n",
       "      <td>hi</td>\n",
       "      <td>3166466058</td>\n",
       "      <td>Long live the jazz drum by adding it to the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4rAQ0LLGrQUKdw6yVXpphPl2n9Pj8xINKm0j</td>\n",
       "      <td>‡§§‡•ã‡§ó‡§º‡§æ‡§ú‡§º‡§æ‡§™‡•ç‡§∞‡§æ‡§á‡§® ‡§µ‡§ø‡§µ‡§æ‡§≤ ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä ‡§ï‡•á ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑ ‡§î‡§∞ ‡§â‡§§‡•ç...</td>\n",
       "      <td>hi</td>\n",
       "      <td>3170967597</td>\n",
       "      <td>Today is the birthday of Akhilesh Yadoji, Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5rwR5EEnrRUd0X6N2Z11T27ZXRKwj9cpoW04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>3184226859</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "0   9rpw0QQErwU5RVAmKGQQIkvlVDXlm7UVQlXN   \n",
       "1   11QA4nnK1AHERwnV6AyyTblPJ0DB1oIN9N7X   \n",
       "2   11QA4nnK1AHERwnV6AyyTblL16BPJeIN9k5J   \n",
       "3   9rpw0QQErwU5RVAmKGQQIkvlVDwnOeFV8Von   \n",
       "4   6P0OR443POT74XKY1x22TWkdLBJ7XDFjojEG   \n",
       "..                                   ...   \n",
       "70  2r7wN33WrwU5Xd0L7PbbIKZx5rYYj1ipErZ2   \n",
       "71  4rAQ0LLGrQUKdw6yVXpphPlydVrAbdCN1bYR   \n",
       "72  6P0OR443POT74XKY1x22TWk8lk7kRYUjpAeL   \n",
       "73  4rAQ0LLGrQUKdw6yVXpphPl2n9Pj8xINKm0j   \n",
       "74  5rwR5EEnrRUd0X6N2Z11T27ZXRKwj9cpoW04   \n",
       "\n",
       "                                                 text lang          id  \\\n",
       "0    ‡§Æ‡•à‡§Ç ‡§§‡•ã ‡§™‡§ø‡§∏‡•ç‡§≤‡•á ‡§¶‡§ø‡§®‡•ã ‡§®‡§ø‡§Ø‡•ã‡§∞‡•ç‡§ï ‡§ú‡§æ‡§∞‡§æ ‡§•‡§æ, ‡§π‡§æ‡§≤‡§æ‡§Ç‡§ï‡§ø ‡§Ø...   hi  3152924803   \n",
       "1    ‡§ú‡§¨ ‡§Ö‡§™‡§®‡•á ‡§®‡§¶‡•á‡§∞‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Ö‡§ú‡§ø‡§Ø‡§æ‡§∞‡§æ ‡§Æ‡•à‡§Ç ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§ï‡§æ ‡§ó‡•ã‡§≤ ‡§∞...   hi  3143801451   \n",
       "2    ⁄©⁄æŸà€Å €ÅŸàŸÜ⁄Üÿßÿ¶€å Ÿæÿ± Ÿæ€åŸπŸÜ€í ÿ≥€í ⁄©ÿ®Ÿàÿ™ÿ± ŸÜ€Å€å⁄∫ ÿ®ŸÜ ÿ¨ÿßÿ™ÿß ÿ®...   ur  3168361509   \n",
       "3                                Thanks for watching!   en  3152593453   \n",
       "4    ‡§Æ‡•á‡§Ç ‡§ó‡•á‡§Ç‡§ó ‡§µ‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§®‡§æ ‡§¨‡§ú‡§Æ‡§æ ‡§∏‡•ã‡§ú ‡§∏‡•á ‡§°‡§∞‡§§‡§æ ‡§ú‡•ã ‡§§‡•Ä‡§∏ ...   hi  3133451477   \n",
       "..                                                ...  ...         ...   \n",
       "70   ‡§ú‡•ã ‡§≤‡§ø‡§è ‡§ñ‡•Å‡§∞‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§ú‡§æ‡§¶‡§æ ‡§¨‡§æ‡§∞ ‡§Ö‡§≤‡•ç‡§≤‡§æ ‡§ï‡§æ ‡§®‡§æ‡§µ ‡§π...   hi  3139094516   \n",
       "71   ÿßŸèÿ™ÿ± Ÿæÿ±ÿØ€åÿ¥ ŸÖ€å⁄∫ ÿ®ÿ±ŸÑ€å ÿ≥€í ÿØŸÑ€å ÿ®ÿ≥ ÿ¨ÿß ÿ±€Å€å €Å€í ÿ®ÿ≥ ÿ±ÿ≥...   ur  3151930999   \n",
       "72   ‡§ú‡§ø‡§®‡•ç‡§¶‡§æ‡§¨‡§æ‡§ú ‡§ú‡§æ‡§ú ‡§¶‡§∞‡§Æ ‡§ï‡•ã ‡§ú‡•ã‡§°‡§º ‡§≠‡§ø‡§ï‡§æ‡§∂ ‡§™‡•á ‡§ì‡§ü ‡§Æ‡§∞‡§æ‡§à ‡§∞‡•á...   hi  3166466058   \n",
       "73   ‡§§‡•ã‡§ó‡§º‡§æ‡§ú‡§º‡§æ‡§™‡•ç‡§∞‡§æ‡§á‡§® ‡§µ‡§ø‡§µ‡§æ‡§≤ ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä ‡§ï‡•á ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑ ‡§î‡§∞ ‡§â‡§§‡•ç...   hi  3170967597   \n",
       "74                                                NaN   en  3184226859   \n",
       "\n",
       "                                          translation  \n",
       "0    I used to live in New York recently, although...  \n",
       "1    When Ajiara is in my eyes, I have been the go...  \n",
       "2    Beating at the top of the nest does not make ...  \n",
       "3                                                      \n",
       "4    I wouldn't have attacked gangs, I would have ...  \n",
       "..                                                ...  \n",
       "70   For which in the Quran, Allah's name is menti...  \n",
       "71   In Uttar Pradesh, the bus was going from Burl...  \n",
       "72   Long live the jazz drum by adding it to the b...  \n",
       "73   Today is the birthday of Akhilesh Yadoji, Pre...  \n",
       "74                                                     \n",
       "\n",
       "[75 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', 'text', 'lang', 'id', 'translation', 'Issues',\n",
       "       'What I Hear', 'Accuracy', 'Short Video Description', 'Text', 'Notes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = pd.read_csv(\"download_csvs/reviewed_sample_audio_translations.csv\")\n",
    "translations = translations.drop(columns=[\"Unnamed: 0\"])\n",
    "translations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "0.4533333333333333\n"
     ]
    }
   ],
   "source": [
    "# Number of accurate translations\n",
    "matched_translations = len(translations[translations[\"What I Hear\"] == \"Match\"])\n",
    "print(matched_translations)\n",
    "print(matched_translations/75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to review 15 seriously\n"
     ]
    }
   ],
   "source": [
    "num_reviewed = len(translations[translations.Issues != \" \"])   # Issues has spaces when there is no issue, so the cells get cut off in csv view\n",
    "print(f\"Had to review {num_reviewed} seriously\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key for Translation Level\n",
    "\n",
    "0 = match\n",
    "\n",
    "1 = changed a few mistranslated words\n",
    "\n",
    "2 = changed a lot of words and meaning\n",
    "\n",
    "3 = fully deciphered the meaning myself, translation was not helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.45\n",
       "1.0    0.29\n",
       "2.0    0.04\n",
       "3.0    0.20\n",
       "Name: Accuracy, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations.Accuracy.value_counts().sort_index().apply(lambda x: round(x/75, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24% of the video's translations did not properly capture the meaning of the video. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import gc\n",
    "from wordsegment import load, segment\n",
    "import re, glob, datetime\n",
    "from langdetect import detect\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from moviepy.editor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from indicnlp import common\n",
    "\n",
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"C:\\\\Users\\\\archi\\\\capstone\\\\indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=\"C:\\\\Users\\\\archi\\\\capstone\\\\indic_nlp_resources\"\n",
    "\n",
    "# Add library to Python path\n",
    "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n",
    "\n",
    "# Set environment variable for resources folder\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp import loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaja mausama achChaa hai. isalie hama aaja khela sakate hai.m!\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "\n",
    "input_text='‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡§≤‡§ø‡§è ‡§π‡§Æ ‡§Ü‡§ú ‡§ñ‡•á‡§≤ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç!'\n",
    "\n",
    "# Transliterate Hindi to Roman\n",
    "print(ItransTransliterator.to_itrans(input_text, 'hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "from cleantext.sklearn import CleanTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindiAlphabetRegex():\n",
    "  numberOfHindiCharacters = 128\n",
    "  hindiAlphabet = []\n",
    "  for i in range(numberOfHindiCharacters):\n",
    "    hindiAlphabet.append(\"\\\\u\" + (\"0%0.2X\" % (0x0900 + i)))\n",
    "  alph = \"\".join(hindiAlphabet)\n",
    "  return \"[\" + alph + \"]\"\n",
    "\n",
    "def containsHindiScript(text):\n",
    "    alphabet = hindiAlphabetRegex()\n",
    "    exp = f\".*{alphabet}.*\"\n",
    "    if re.match(exp, text):\n",
    "        return True\n",
    "    return False    # because false can mean it contains punjabi or something\n",
    "\n",
    "# U+0A80..U+0AFF (128 code points)\n",
    "def gujaratiAlphabetRegex():\n",
    "  alphabet = []\n",
    "  for i in range(128):\n",
    "    alphabet.append(\"\\\\u\" + (\"0%0.2X\" % (0x0A80 + i)))\n",
    "  alph = \"\".join(alphabet)\n",
    "  return \"[\" + alph + \"]\"\n",
    "\n",
    "def containsGujaratiScript(text):\n",
    "    alphabet = gujaratiAlphabetRegex()\n",
    "    exp = f\".*{alphabet}.*\"\n",
    "    if re.match(exp, text):\n",
    "        return True\n",
    "    return False    # because false can mean it contains punjabi or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption_lang(df):\n",
    "    df['has_hindi_script'] = df['clean_caption'].apply(containsHindiScript)\n",
    "    df['has_gujarati_script'] = df['clean_caption'].apply(containsGujaratiScript)\n",
    "    df['is_english'] = df['clean_caption'].apply(lambda x: x.isascii())\n",
    "    df['is_empty'] = df['clean_caption'].str.match(r\"^$\")\n",
    "\n",
    "    conditions = [\n",
    "        (df['is_empty'] == True),\n",
    "        (df['is_english'] == True),\n",
    "        (df['has_hindi_script'] == True),\n",
    "        (df['has_gujarati_script'] == True),\n",
    "        (df['has_gujarati_script'] == False) & (df['has_hindi_script'] == False) & (df['is_english'] == False),  \n",
    "    ]\n",
    "\n",
    "    values = ['empty', 'en', 'hi', 'gj', 'other']\n",
    "\n",
    "    df['caption_lang'] = np.select(conditions, values)\n",
    "\n",
    "    df.drop(columns=['has_hindi_script', 'has_gujarati_script', 'is_english', 'is_empty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"creds.txt\", \"r\") as credsfile:\n",
    "    username = credsfile.readline().strip()\n",
    "    password = credsfile.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get posts from database\n",
    "db_url = f'postgresql://{username}:{password}@localhost:5432/moj'\n",
    "\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "dbConnection = engine.connect();\n",
    "\n",
    "query = text('select * from \\\"posts\\\" where post_month = 6 OR (post_month = 7 and post_day < 15)')\n",
    "\n",
    "df = pd.read_sql(query, dbConnection);\n",
    "\n",
    "dbConnection.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean characters, etc from captions\n",
    "combined = df\n",
    "\n",
    "cleaner = CleanTransformer(no_punct = True, \n",
    "                           lower=True, \n",
    "                           no_emoji=True, \n",
    "                           no_line_breaks=True, \n",
    "                           no_urls=True, \n",
    "                           normalize_whitespace=True,\n",
    "                           to_ascii=False)\n",
    "\n",
    "exp_remove_hashtags = \"#+[^\\s]+\"\n",
    "exp_remove_mentions = \"@+[^\\s]+\"\n",
    "\n",
    "cleaned_without_mentions = combined.c.str.replace(exp_remove_mentions, \"\")\n",
    "cleaned_without_hashtags = cleaned_without_mentions.str.replace(exp_remove_hashtags, \"\")\n",
    "cleaned_final = cleaner.transform(cleaned_without_hashtags)\n",
    "combined['clean_caption'] = cleaned_final\n",
    "\n",
    "punct_to_remove = ['$', '+', '<', '=', '>', '^', '`', '|', '~']\n",
    "# extra cleaning. because these characters were missed for some reason\n",
    "for punct in punct_to_remove:\n",
    "    combined.clean_caption = combined.clean_caption.str.replace(punct, '')\n",
    "combined.clean_caption = combined.clean_caption.str.replace(\"\\s+\", ' ')     # normalize white space\n",
    "combined.clean_caption = combined.clean_caption.str.strip()\n",
    "combined.reset_index(inplace=True)\n",
    "\n",
    "get_caption_lang(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_captions():\n",
    "    sample_captions = combined['clean_caption'].sample(50)\n",
    "    non_empty_captions = sample_captions[(sample_captions.str.match(r\"^$\") == False) & (sample_captions.apply(isEnglish))]\n",
    "    print(len(non_empty_captions), \" captions\")\n",
    "    return non_empty_captions\n",
    "\n",
    "def write_to_LID(captions, filename = \"captions.txt\"):\n",
    "    # now lets write these to a txt file in the necessary format\n",
    "    \n",
    "    with open(f\"C:\\\\Users\\\\archi\\\\capstone\\\\LID-tool\\\\{filename}\", \"w\") as outfile:\n",
    "        for i,caption in enumerate(captions):\n",
    "            outfile.write(f\"{i}\\t{caption}\\n\")\n",
    "\n",
    "# go to command prompt \\LID-tools and run python getLanguage.py captions.txt\n",
    "\n",
    "def read_LID_results(filename = \"captions.txt_tagged\"):\n",
    "    # run python script in the cmd bc issues with jupyter \n",
    "    with open(f\"C:\\\\Users\\\\archi\\\\capstone\\\\LID-tool\\\\{filename}\", \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "\n",
    "    lid_res = lines[1::2]  # classified captions\n",
    "\n",
    "    lid_words = list() \n",
    "    for caption in lid_res:\n",
    "        words = caption.split()[1:]\n",
    "        word_groups = list()\n",
    "        for word in words:\n",
    "            word_groups.append(word.split(\"/\"))\n",
    "        lid_words.append(word_groups)\n",
    "    # better way would be to separate all words of all cleaned captions and then add the classifications\n",
    "\n",
    "    return lid_words\n",
    "\n",
    "def transliterateCaption(classified_caption):\n",
    "    # writing a function that does all the work in one go so i can test how it works on diff sampled captions\n",
    "\n",
    "    # classified_caption is like [ [word, HI] ...]\n",
    "    # get the longest substring\n",
    "    # classified_caption = []\n",
    "    \n",
    "    lang = 1\n",
    "\n",
    "    max_hindi_lens = [ 1 if classified_caption[0][lang] == 'HI' else 0]\n",
    "\n",
    "    for i, word in enumerate(classified_caption):\n",
    "        if i == 0: \n",
    "            continue\n",
    "        if word[lang] == 'HI':\n",
    "            max_hindi_lens.append(max_hindi_lens[i-1]+1)\n",
    "        else:\n",
    "            max_hindi_lens.append(0)\n",
    "\n",
    "    np_max_hindi_lens = np.array(max_hindi_lens)\n",
    "\n",
    "    if np_max_hindi_lens.max() > 0:\n",
    "        end_of_str = np_max_hindi_lens.argmax()\n",
    "        start_of_str = end_of_str - max_hindi_lens[end_of_str] + 1\n",
    "        all_words = [ word[0] for word in classified_caption[start_of_str:end_of_str+1]]\n",
    "        final_str = \" \".join(all_words)\n",
    "\n",
    "        print(final_str)\n",
    "\n",
    "        caption_to_script = ItransTransliterator.from_itrans(final_str, 'hi')\n",
    "        print(\"Option 1\")\n",
    "        print(caption_to_script)\n",
    "    else:\n",
    "        print(\"All english\")\n",
    "\n",
    "    # second option is to individually translate each word and put the script together\n",
    "\n",
    "    caption_words_2 = list()\n",
    "    for classified_word in classified_caption:\n",
    "        if classified_word[lang] == 'HI':\n",
    "            word_script = ItransTransliterator.from_itrans(classified_word[0], 'hi')\n",
    "            caption_words_2.append(word_script)\n",
    "        else:\n",
    "            caption_words_2.append(classified_word[0])\n",
    "    caption_to_script_2 = \" \".join(caption_words_2)\n",
    "\n",
    "    print(\"Option 2\")\n",
    "    print(caption_to_script_2)\n",
    "\n",
    "def create_roman_hindi_list(classified_captions):\n",
    "    all_words = list()\n",
    "\n",
    "    word_index = 0\n",
    "    lang_index = 1\n",
    "\n",
    "    for caption in classified_captions:\n",
    "        for word_group in caption:\n",
    "            if word_group[lang_index] == 'HI':\n",
    "                all_words.append(word_group[word_index])\n",
    "\n",
    "    roman_hindi_list = pd.Series(all_words).unique()\n",
    "\n",
    "    print(f\"{len(roman_hindi_list)}/{len(all_words)} unique\")\n",
    "\n",
    "    return roman_hindi_list\n",
    "\n",
    "def transliterate_word(x):\n",
    "    return ItransTransliterator.from_itrans(x, 'hi')\n",
    "\n",
    "# Imports the Google Cloud Translation library\n",
    "from google.cloud import translate\n",
    "\n",
    "# Initialize Translation client\n",
    "def detect_language(text=\"‡§∏‡§∞‡•ç‡§ï‡§æ‡§∞\"):\n",
    "\n",
    "    try:\n",
    "               \n",
    "        project_id=\"agile-tracker-398919\"\n",
    "        client = translate.TranslationServiceClient()\n",
    "\n",
    "        location = \"global\"\n",
    "\n",
    "        parent = f\"projects/{project_id}/locations/{location}\"\n",
    "\n",
    "        response = client.detect_language(parent=f\"projects/{project_id}/locations/global\", \n",
    "                                          content=text)\n",
    "       \n",
    "\n",
    "        # Display the translation for each input text provided\n",
    "        for language in response.languages:\n",
    "            return language.language_code\n",
    "        \n",
    "    except:\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify English/roman Hindi captions\n",
    "english_captions = combined[combined.caption_lang == 'en']\n",
    "len_captions = english_captions.clean_caption.apply(lambda x: len(x.split()))\n",
    "captions_to_exclude = len_captions.nlargest(4).index   # these have about 300 words or more - LID-tool freezes\n",
    "english_captions.drop(captions_to_exclude, inplace=True)    # I will leave the 4 captions as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions_0_1000.txt\n",
      "captions_1000_1571.txt\n"
     ]
    }
   ],
   "source": [
    "# Run LID in groups of 1000 to separate roman Hindi and English words in the english captions\n",
    "\n",
    "lid_final_files = list()\n",
    "\n",
    "i = 0\n",
    "while i < len(english_captions):\n",
    "    start = i\n",
    "    end = i + 1000\n",
    "    if end > len(english_captions):\n",
    "        end = len(english_captions)\n",
    "\n",
    "    filename = f\"captions_{start}_{end}.txt\"\n",
    "    lid_final_files.append(filename)\n",
    "    print(filename)\n",
    "    \n",
    "    captions = english_captions.iloc[start:end].clean_caption\n",
    "    write_to_LID(captions, filename=filename)\n",
    "\n",
    "    i = end\n",
    "    \n",
    "    # cd capstone\\LID-tool\n",
    "    # set MALLET_HOME=C:\\Users\\archi\\capstone\\LID-tool\\mallet-2.0.8\\\n",
    "    # python getLanguage.py captions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/3129 unique\n",
      "865/1995 unique\n"
     ]
    }
   ],
   "source": [
    "# Collect all the Roman hindi words identified\n",
    "all_roman_hindi_words = list()\n",
    "for filename in lid_final_files:\n",
    "    classified_captions = read_LID_results(filename=f\"{filename}_tagged\")\n",
    "    roman_hindi_list = create_roman_hindi_list(classified_captions)\n",
    "    all_roman_hindi_words.append(roman_hindi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transliterate and translate all Roman hindi words\n",
    "final_rh_words = np.unique(np.concatenate(all_roman_hindi_words))\n",
    "len(final_rh_words)\n",
    "roman_hindi_translations = pd.DataFrame({\"word\": final_rh_words})\n",
    "roman_hindi_translations['hindi_word'] = roman_hindi_translations.word.apply(transliterate_word)\n",
    "roman_hindi_translations[\"translation\"] = roman_hindi_translations.hindi_word.apply(translate_text)\n",
    "roman_hindi_translations.to_csv(\"translations/roman_hindi_captions_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate all Hindi captions\n",
    "hindi_captions = combined[combined.caption_lang == 'hi']\n",
    "hindi_translation = hindi_captions.clean_caption.apply(translate_text)\n",
    "final_hindi_translations = pd.DataFrame({\"index\": hindi_captions[\"index\"], \n",
    "                                         \"i\": hindi_captions[\"i\"],\n",
    "                                         \"clean_caption\": hindi_captions['clean_caption'], \n",
    "                                         \"hi_translated_caption\": hindi_translation})\n",
    "final_hindi_translations.to_csv(\"translations/hindi_captions_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate all other language captions\n",
    "other_captions = combined[combined.caption_lang == 'other']\n",
    "len(other_captions)\n",
    "other_captions[\"google_lang\"] = other_captions.clean_caption.apply(detect_language)\n",
    "# to avoid the slicing issues\n",
    "other_captions_df = pd.DataFrame({\n",
    "    \"i\": other_captions.i,\n",
    "    \"caption\": other_captions.clean_caption, \n",
    "    \"caption_lang\": other_captions.caption_lang, \n",
    "    \"google_lang\": other_captions.google_lang, \n",
    "    \"translation\": \"\", # this is what we'll fill in\n",
    "})\n",
    "\n",
    "for i in other_captions_df.index:\n",
    "    row = other_captions_df.loc[i]\n",
    "    other_captions_df.loc[i, \"translation\"] = translate_text(row.caption, row.google_lang)     # to avoid slice setting error use .loc[i, \"\"]\n",
    "other_captions_df.to_csv(\"translations/other_captions_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean any false RH translations (using work from thesis)\n",
    "prev_rh = pd.read_csv(\"../clean/captions/reviewed/roman_hindi_final.csv\")\n",
    "rh_words_df = pd.read_csv(\"translations/roman_hindi_captions_final.csv\")\n",
    "rh_merged = rh_words_df.merge(prev_rh, how='left', left_on=\"word\", right_on=\"word\").drop(columns=[\"Unnamed: 0_y\", \"hindi_word_y\", \"translation_y\"])\n",
    "rh_merged.columns=[\"Unnamed: 0\", \"word\", \"hindi_word\", \"translation\", \"translation_reviewed\"]\n",
    "rh_merged['translation_reviewed'] = np.where(rh_merged['translation_reviewed'].isna(), rh_merged['translation'], rh_merged['translation_reviewed'])\n",
    "rh_words_df = rh_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them all\n",
    "combined['translation'] = \"\"    # blank to fill in\n",
    "hindi_final_df = pd.read_csv(\"translations/hindi_captions_final.csv\").rename(columns={\"Unnamed: 0\": \"combined_index\"})\n",
    "other_final_df = pd.read_csv(\"translations/other_captions_final.csv\").rename(columns={\"Unnamed: 0\": \"combined_index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_final_df[\"caption_lang\"] = \"hi\"\n",
    "hindi_final_df[\"translation\"] = hindi_final_df[\"hi_translated_caption\"]\n",
    "hindi_final_df.i = hindi_final_df.i.astype('str')\n",
    "hindi_final_df = hindi_final_df[[\"i\", \"caption_lang\", \"clean_caption\", \"translation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_final_df[\"clean_caption\"] = other_final_df[\"caption\"]\n",
    "other_final_df[\"caption_lang\"] = other_final_df[\"google_lang\"]\n",
    "other_final_df.i = other_final_df.i.astype('str')\n",
    "other_final_df = other_final_df[[\"i\", \"caption_lang\", \"clean_caption\", \"translation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_final_df = combined[combined.caption_lang == \"empty\"][[\"i\", \"caption_lang\", \"clean_caption\"]]\n",
    "empty_final_df[\"translation\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined the Roman hindi and english word translations\n",
    "eng_cap_exploded = english_captions.clean_caption.str.split().explode().reset_index().rename(columns={\"clean_caption\":\"word\"})\n",
    "\n",
    "joined = eng_cap_exploded.merge(rh_words_df, how=\"outer\", on=\"word\")\n",
    "joined['translation_reviewed'] = np.where(\n",
    "    joined['translation_reviewed'].isna(), \n",
    "    joined['word'], \n",
    "    joined['translation_reviewed'])\n",
    "\n",
    "joined.dropna(subset=[\"index\"], inplace=True)\n",
    "\n",
    "string_punct = '.!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "final_res = joined.groupby('index')['translation_reviewed'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "final_res['translation_reviewed'] = final_res.translation_reviewed.str.lower().str.strip(string_punct).str.replace(\".\", \"\")\n",
    "\n",
    "english_final_df = english_captions.merge(final_res, how=\"left\", left_on=\"index\", right_on=\"index\")\n",
    "english_final_df = english_final_df[[\"i\", \"caption_lang\", \"clean_caption\", \"translation_reviewed\"]]\n",
    "english_final_df.columns = [\"i\", \"caption_lang\", \"clean_caption\", \"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_en_spam_df = combined.iloc[captions_to_exclude][[\"i\", \"clean_caption\", \"caption_lang\"]]\n",
    "excluded_en_spam_df[\"translation\"] = \"\" # does not preserving the original caption mess with the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_caption_translations = pd.concat([english_final_df, hindi_final_df, other_final_df, empty_final_df, excluded_en_spam_df], axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_caption_translations.to_csv(\"download_csvs/all_posts_caption_translations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>caption_lang</th>\n",
       "      <th>clean_caption</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3185854507</td>\n",
       "      <td>en</td>\n",
       "      <td>hindu status ll ‚Äç hindutva shree ram status ‚Äç ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3185853920</td>\n",
       "      <td>en</td>\n",
       "      <td>hindu status ll ‚Äç hindutva shree ram status ‚Äç ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3154016206</td>\n",
       "      <td>en</td>\n",
       "      <td>ùôµùöòùöïùöïùöòùö† ùôªùöíùöîùöé ùô≤ùöòùöñùöñùöéùöóùöù ùöÇùöëùöäùöõùöé ùöÇùöäùöüùöé ùöëùöäùöúùöëùöùùöäùöê</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3166639289</td>\n",
       "      <td>en</td>\n",
       "      <td>ùêìùê≤ùê©ùêû ùêâùêöùê¢ ùêáùê¢ùêßùêù ùê¢ùêü ùê≤ùê®ùêÆ ùê•ùê®ùêØùêû ùê≠ùê°ùê¢ùê¨ ùë≥ùë∞ùë≤ùë¨ ùë™ùë∂ùë¥ùë¥ùë¨ùëµùëª ùë∫ùëØ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3161079888</td>\n",
       "      <td>en</td>\n",
       "      <td>music follow for more updates ‚òÖ like comments ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>3159356003</td>\n",
       "      <td>en</td>\n",
       "      <td>‚Äç‚Äç</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>3170675487</td>\n",
       "      <td>en</td>\n",
       "      <td>‚óè fact about pm modi pm modi grand welcome in ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3180500808</td>\n",
       "      <td>en</td>\n",
       "      <td>yogi yoga like namaz ‚Äç‚Äç</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>3170474184</td>\n",
       "      <td>en</td>\n",
       "      <td>correct way to chant om Ô∏è aum Ô∏è</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>3157891937</td>\n",
       "      <td>en</td>\n",
       "      <td>this international yoga day‚Äç take a step towar...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3137446339</td>\n",
       "      <td>en</td>\n",
       "      <td>2 days free yoga workshop ‚Äç‚Äç by url whatsapp g...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             i caption_lang  \\\n",
       "32  3185854507           en   \n",
       "36  3185853920           en   \n",
       "37  3154016206           en   \n",
       "44  3166639289           en   \n",
       "49  3161079888           en   \n",
       "66  3159356003           en   \n",
       "78  3170675487           en   \n",
       "79  3180500808           en   \n",
       "80  3170474184           en   \n",
       "81  3157891937           en   \n",
       "83  3137446339           en   \n",
       "\n",
       "                                        clean_caption translation  \n",
       "32  hindu status ll ‚Äç hindutva shree ram status ‚Äç ...         NaN  \n",
       "36  hindu status ll ‚Äç hindutva shree ram status ‚Äç ...         NaN  \n",
       "37             ùôµùöòùöïùöïùöòùö† ùôªùöíùöîùöé ùô≤ùöòùöñùöñùöéùöóùöù ùöÇùöëùöäùöõùöé ùöÇùöäùöüùöé ùöëùöäùöúùöëùöùùöäùöê         NaN  \n",
       "44  ùêìùê≤ùê©ùêû ùêâùêöùê¢ ùêáùê¢ùêßùêù ùê¢ùêü ùê≤ùê®ùêÆ ùê•ùê®ùêØùêû ùê≠ùê°ùê¢ùê¨ ùë≥ùë∞ùë≤ùë¨ ùë™ùë∂ùë¥ùë¥ùë¨ùëµùëª ùë∫ùëØ...         NaN  \n",
       "49  music follow for more updates ‚òÖ like comments ...         NaN  \n",
       "66                                                 ‚Äç‚Äç         NaN  \n",
       "78  ‚óè fact about pm modi pm modi grand welcome in ...         NaN  \n",
       "79                            yogi yoga like namaz ‚Äç‚Äç         NaN  \n",
       "80                    correct way to chant om Ô∏è aum Ô∏è         NaN  \n",
       "81  this international yoga day‚Äç take a step towar...         NaN  \n",
       "83  2 days free yoga workshop ‚Äç‚Äç by url whatsapp g...         NaN  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_caption_translations[final_caption_translations.translation.isna()]\n",
    "# special characters caused these not to translate correctly. doing manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>i</th>\n",
       "      <th>caption_lang</th>\n",
       "      <th>clean_caption</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>66</td>\n",
       "      <td>3159356003</td>\n",
       "      <td>en</td>\n",
       "      <td>‚Äç‚Äç</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>6</td>\n",
       "      <td>3177672823</td>\n",
       "      <td>empty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>11</td>\n",
       "      <td>3147123875</td>\n",
       "      <td>empty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>14</td>\n",
       "      <td>3178528842</td>\n",
       "      <td>empty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>15</td>\n",
       "      <td>3179268817</td>\n",
       "      <td>empty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>3209</td>\n",
       "      <td>3139894306</td>\n",
       "      <td>empty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>3102</td>\n",
       "      <td>3164772661</td>\n",
       "      <td>en</td>\n",
       "      <td>second most liked instagram hashtags used with...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>1001</td>\n",
       "      <td>3179810500</td>\n",
       "      <td>en</td>\n",
       "      <td>srcreationo srcreation0231 sr creation 0296 sr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>408</td>\n",
       "      <td>3179759267</td>\n",
       "      <td>en</td>\n",
       "      <td>srcreation0231 sr creation 0296 sr creation 03...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>2426</td>\n",
       "      <td>3179789053</td>\n",
       "      <td>en</td>\n",
       "      <td>srcreation0231 sr creation 0296 sr creation 03...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>737 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0           i caption_lang  \\\n",
       "2460          66  3159356003           en   \n",
       "2485           6  3177672823        empty   \n",
       "2486          11  3147123875        empty   \n",
       "2487          14  3178528842        empty   \n",
       "2488          15  3179268817        empty   \n",
       "...          ...         ...          ...   \n",
       "3216        3209  3139894306        empty   \n",
       "3217        3102  3164772661           en   \n",
       "3218        1001  3179810500           en   \n",
       "3219         408  3179759267           en   \n",
       "3220        2426  3179789053           en   \n",
       "\n",
       "                                          clean_caption translation  \n",
       "2460                                                 ‚Äç‚Äç         NaN  \n",
       "2485                                                NaN         NaN  \n",
       "2486                                                NaN         NaN  \n",
       "2487                                                NaN         NaN  \n",
       "2488                                                NaN         NaN  \n",
       "...                                                 ...         ...  \n",
       "3216                                                NaN         NaN  \n",
       "3217  second most liked instagram hashtags used with...         NaN  \n",
       "3218  srcreationo srcreation0231 sr creation 0296 sr...         NaN  \n",
       "3219  srcreation0231 sr creation 0296 sr creation 03...         NaN  \n",
       "3220  srcreation0231 sr creation 0296 sr creation 03...         NaN  \n",
       "\n",
       "[737 rows x 5 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf = pd.read_csv(\"download_csvs/all_posts_caption_translations.csv\")\n",
    "finaldf[finaldf.translation.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do something like this to read/add to the entire dataset\n",
    "# df.merge(final_caption_translations, how=\"left\", left_on=\"i\", right_on=\"i\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
