{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Facial Recognition\n",
        "This code was run in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_seNNHDZOWXI",
        "outputId": "86a777ae-f668-4664-a337-229ee64aa5c4"
      },
      "outputs": [],
      "source": [
        "!pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLnSvDuuW4jO"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from imutils import paths\n",
        "import face_recognition\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prHmsz7ck6j1"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"dataset\": \"dataset\",\n",
        "    \"detection_method\": \"cnn\",\n",
        "    \"encodings\": \"encodings.pickle\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmGrB1mGk3RS",
        "outputId": "d8eb2646-70cb-45b5-9285-29c2bac0c71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] quantifying faces...\n",
            "['dataset/kejriwal/kejriwal_3.jpg', 'dataset/kejriwal/kejriwal_5.jpg', 'dataset/kejriwal/kejriwal_4.jpg', 'dataset/kejriwal/kejriwal_6.jpg', 'dataset/kejriwal/kejriwal_2.jpg', 'dataset/kejriwal/kejriwal_1.jpg', 'dataset/yadav/yadav_1.jpg', 'dataset/yadav/yadav_5.jpg', 'dataset/yadav/yadav_2.jpg', 'dataset/yadav/yadav_3.jpg', 'dataset/yadav/yadav_4.jpg', 'dataset/owaisi/owaisi_5.jpg', 'dataset/owaisi/owaisi_4.jpg', 'dataset/owaisi/owaisi_1.jpg', 'dataset/owaisi/owaisi_3.jpg', 'dataset/owaisi/owaisi_2.jpg', 'dataset/modi/modi_5.jpg', 'dataset/modi/modi_3.jpg', 'dataset/modi/modi_2.jpg', 'dataset/modi/modi_1.jpg', 'dataset/modi/modi_4.jpg', 'dataset/modi/modi_6.jpg', 'dataset/gandhi/gandhi_3.jpg', 'dataset/gandhi/gandhi_1.jpg', 'dataset/gandhi/gandhi_4.jpg', 'dataset/gandhi/gandhi_5.jpg', 'dataset/gandhi/gandhi_6.jpg', 'dataset/gandhi/gandhi_2.jpg']\n"
          ]
        }
      ],
      "source": [
        "# grab the paths to the input images in our dataset\n",
        "print(\"[INFO] quantifying faces...\")\n",
        "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
        "# initialize the list of known encodings and known names\n",
        "knownEncodings = []\n",
        "knownNames = []\n",
        "\n",
        "print(imagePaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QaYDl-ykrFF"
      },
      "outputs": [],
      "source": [
        "# loop over the image paths\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "\t# extract the person name from the image path\n",
        "\tprint(\"[INFO] processing image {}/{}\".format(i + 1,\n",
        "\t\tlen(imagePaths)))\n",
        "\tname = imagePath.split(os.path.sep)[-2]\n",
        "\t# load the input image and convert it from BGR (OpenCV ordering)\n",
        "\t# to dlib ordering (RGB)\n",
        "\timage = cv2.imread(imagePath)\n",
        "\trgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\t# detect the (x, y)-coordinates of the bounding boxes\n",
        "\t# corresponding to each face in the input image\n",
        "\tboxes = face_recognition.face_locations(rgb,\n",
        "\t\tmodel=args[\"detection_method\"])\n",
        "\t# compute the facial embedding for the face\n",
        "\tencodings = face_recognition.face_encodings(rgb, boxes)\n",
        "\t# loop over the encodings\n",
        "\tfor encoding in encodings:\n",
        "\t\t# add each encoding + name to our set of known names and\n",
        "\t\t# encodings\n",
        "\t\tknownEncodings.append(encoding)\n",
        "\t\tknownNames.append(name)\n",
        "\n",
        "# dump the facial encodings + names to disk\n",
        "print(\"[INFO] serializing encodings...\")\n",
        "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
        "f = open(args[\"encodings\"], \"wb\")\n",
        "f.write(pickle.dumps(data))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from imutils.video import FileVideoStream\n",
        "import face_recognition\n",
        "import imutils\n",
        "import pickle\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "basepath = \"drive/MyDrive/Capstone/Videos/videos_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_paths = pd.read_csv(\"drive/MyDrive/Capstone/fr_sample.csv\").filepath.str.replace(\"video_\", \"videos_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_data = pd.DataFrame(columns=[\"filename\", \"names\", \"counts\"])\n",
        "\n",
        "args = {\n",
        "\t\"encodings\": \"encodings.pickle\",\n",
        "\t\"display\": 0,\n",
        "\t\"detection_method\": \"cnn\",\n",
        "\t\"output\": None,\n",
        "}\n",
        "\n",
        "# load the known faces and embeddings\n",
        "print(\"[INFO] loading encodings...\")\n",
        "data = pickle.loads(open(args[\"encodings\"], \"rb\").read())\n",
        "\n",
        "# for file_num, filename in enumerate(ordered_files[85:100]):\n",
        "for file_num, filename in enumerate(file_paths[85:]):\n",
        "\tif filename == \".ipynb_checkpoints\":\n",
        "\t\tcontinue\n",
        "\tstarttime = datetime.now()\n",
        "\n",
        "\t# args[\"input\"] = f\"{basepath}/{filename}\"\n",
        "\targs[\"input\"] = filename\n",
        "\n",
        "\t# initialize the video stream and pointer to output video file, then\n",
        "\t# allow the camera sensor to warm up\n",
        "\n",
        "\tprint(f\"[INFO] starting video stream for {filename}...\")\n",
        "\n",
        "\tvs = FileVideoStream(args[\"input\"]).start()\n",
        "\n",
        "\twriter = None\n",
        "\ttime.sleep(2.0)\n",
        "\tnum_frames_looped=0\n",
        "\n",
        "\t# loop over frames from the video file stream\n",
        "\twhile True:\n",
        "\t\t# grab the frame from the threaded video stream\n",
        "\t\tframe = vs.read()\n",
        "\t\tif frame is None:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\twhile num_frames_looped % 10 != 0:\n",
        "\t\t\tframe = vs.read()\n",
        "\t\t\tif frame is None:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tnum_frames_looped+=1\n",
        "\t\tnum_frames_looped+=1\n",
        "\t\tprint(f\"on frame {num_frames_looped}\")\n",
        "\t\t# convert the input frame from BGR to RGB then resize it to have\n",
        "\t\t# a width of 750px (to speedup processing)\n",
        "\t\ttry:\n",
        "\t\t\trgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\t\texcept:\n",
        "\t\t\tprint(f\"breaking after {num_frames_looped}\")\n",
        "\t\t\tbreak\n",
        "\t\trgb = imutils.resize(frame, width=750)\n",
        "\t\tr = frame.shape[1] / float(rgb.shape[1])\n",
        "\t\t# detect the (x, y)-coordinates of the bounding boxes\n",
        "\t\t# corresponding to each face in the input frame, then compute\n",
        "\t\t# the facial embeddings for each face\n",
        "\t\tboxes = face_recognition.face_locations(rgb, model=args[\"detection_method\"])\n",
        "\t\tencodings = face_recognition.face_encodings(rgb, boxes)\n",
        "\t\tnames = []\n",
        "\n",
        "\t\t\t# loop over the facial embeddings\n",
        "\t\tfor encoding in encodings:\n",
        "\t\t\t# attempt to match each face in the input image to our known\n",
        "\t\t\t# encodings\n",
        "\t\t\tmatches = face_recognition.compare_faces(data[\"encodings\"],\n",
        "\t\t\t\tencoding)\n",
        "\t\t\tname = \"Unknown\"\n",
        "\t\t\t# check to see if we have found a match\n",
        "\t\t\tif True in matches:\n",
        "\t\t\t\t# find the indexes of all matched faces then initialize a\n",
        "\t\t\t\t# dictionary to count the total number of times each face\n",
        "\t\t\t\t# was matched\n",
        "\t\t\t\tmatchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
        "\t\t\t\tcounts = {}\n",
        "\t\t\t\t# loop over the matched indexes and maintain a count for\n",
        "\t\t\t\t# each recognized face face\n",
        "\t\t\t\tfor i in matchedIdxs:\n",
        "\t\t\t\t\tname = data[\"names\"][i]\n",
        "\t\t\t\t\tcounts[name] = counts.get(name, 0) + 1\n",
        "\t\t\t\t# determine the recognized face with the largest number\n",
        "\t\t\t\t# of votes (note: in the event of an unlikely tie Python\n",
        "\t\t\t\t# will select first entry in the dictionary)\n",
        "\t\t\t\tname = max(counts, key=counts.get)\n",
        "\n",
        "\t\t\t# update the list of names\n",
        "\t\t\tnames.append(name)\n",
        "\t\t\t\t# loop over the recognized faces\n",
        "\n",
        "\t\ttry:\n",
        "\t\t\tvideo_data.loc[file_num] = [filename[:-4], str(names), json.dumps(counts)]\n",
        "\t\texcept:\n",
        "\t\t\tvideo_data.loc[file_num] = [filename[:-4], str(names), '{}']\n",
        "\t\tvideo_data.to_csv(f\"drive/MyDrive/Capstone/fr_output/fr_group_2_85_100.csv\")\n",
        "\t# do a bit of cleanup\n",
        "\tcv2.destroyAllWindows()\n",
        "\tvs.stop()\n",
        "\tprint(datetime.now() - starttime)\n",
        "\n",
        "video_data.to_csv(f\"drive/MyDrive/Capstone/fr_output/fr_group_2__85_100.csv\")\n",
        "video_data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
